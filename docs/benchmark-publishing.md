# Benchmark Publishing to GitHub Pages

This document explains how the project automatically generates and publishes performance benchmarks with flamegraphs to GitHub Pages.

## Overview

The benchmark publishing system consists of:

1. **Criterion.rs benchmarks** with pprof flamegraph integration
2. **Post-processing script** to add flamegraph links to HTML reports
3. **GitHub Actions workflow** for automated publishing
4. **GitHub Pages** hosting for public access

## Components

### 1. Benchmark Configuration

The benchmarks are configured in `benches/bitemporal_benchmarks.rs` with pprof integration:

```rust
use pprof::criterion::{Output, PProfProfiler};

fn profiled() -> Criterion {
    Criterion::default().with_profiler(PProfProfiler::new(100, Output::Flamegraph(None)))
}

criterion_group! {
    name = benches;
    config = profiled();
    targets = bench_small_dataset, bench_medium_dataset, // ...
}
```

### 2. Flamegraph Generation

Flamegraphs are generated by running benchmarks with the `--profile-time` flag:

```bash
# Generate flamegraph for medium dataset (5 seconds of profiling)
cargo bench --bench bitemporal_benchmarks medium_dataset -- --profile-time 5
```

Output location: `target/criterion/<benchmark_name>/profile/flamegraph.svg`

### 3. HTML Integration Script

The `scripts/add_flamegraphs_to_html.py` script post-processes Criterion's HTML reports to add flamegraph links:

```python
# Adds flamegraph links to the "Additional Plots" section
python3 scripts/add_flamegraphs_to_html.py target/criterion
```

This modifies the HTML reports to include:
```html
<li>
    <a href="../profile/flamegraph.svg">üî• Flamegraph</a>
</li>
```

### 4. GitHub Actions Workflow

The `.github/workflows/benchmarks.yml` workflow:

1. **Runs benchmarks** with flamegraph generation for key tests
2. **Post-processes HTML** to add flamegraph links
3. **Creates landing page** with benchmark overview
4. **Publishes to GitHub Pages** automatically

### 5. Benchmark Categories

The system generates reports for:

- **Core Benchmarks**:
  - `small_dataset` - Basic functionality (5 records)
  - `medium_dataset` - Medium scale (100 records) + flamegraph
  - `conflation_effectiveness` - Adjacent segment merging + flamegraph

- **Scaling Analysis**:
  - Dataset size scaling (10 ‚Üí 500k records)
  - Large dataset flamegraph (500k records)
  - Parallel effectiveness analysis

## Local Testing

### Generate Benchmarks Locally

```bash
# Generate flamegraphs for key benchmarks
cargo bench --bench bitemporal_benchmarks medium_dataset -- --profile-time 5
cargo bench --bench bitemporal_benchmarks conflation_effectiveness -- --profile-time 5
cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/500000" -- --profile-time 5

# Add flamegraph links to HTML reports
python3 scripts/add_flamegraphs_to_html.py

# Serve locally for testing
python3 -m http.server 8000 --directory target/criterion
```

Then visit: http://localhost:8000/report/

### Test GitHub Pages Setup

```bash
# Prepare pages directory (mimics GitHub Actions)
mkdir -p pages
cp -r target/criterion/* pages/

# Serve GitHub Pages content locally
python3 -m http.server 8000 --directory pages
```

## GitHub Pages Setup

### Prerequisites

1. **Enable GitHub Pages** in repository settings:
   - Go to Settings ‚Üí Pages
   - Source: "GitHub Actions"

2. **Required permissions** in workflow:
   ```yaml
   permissions:
     contents: read
     pages: write
     id-token: write
   ```

### Workflow Triggers

The benchmark workflow runs on:
- Push to `main`/`master` branch
- Pull requests to `main`/`master` branch  
- Manual workflow dispatch

### Expected Output

After successful deployment, GitHub Pages will host:

- **Landing page** at `https://<username>.github.io/<repo>/` 
- **Individual benchmark reports** with integrated flamegraph links
- **Flamegraph SVG files** directly accessible

## Performance Expectations

Based on current benchmarking:

| Benchmark | Expected Time | Flamegraph |
|-----------|---------------|------------|
| Small Dataset (5 records) | ~30-35 ¬µs | ‚ùå |
| Medium Dataset (100 records) | ~165-170 ¬µs | ‚úÖ |
| Large Dataset (500k records) | ~900-950 ms | ‚úÖ |
| Conflation Effectiveness | ~28 ¬µs | ‚úÖ |

## Flamegraph Analysis

Flamegraphs help identify:

- **Hot functions**: Wide bars show time-consuming operations
- **Call patterns**: Stack depth shows function call chains  
- **Parallel effectiveness**: Compare serial vs parallel sections
- **Memory allocations**: Identify allocation hotspots

### Key Areas to Monitor

1. **`process_id_timeline`** - Core algorithm logic
2. **Rayon parallel processing** - Parallelization overhead
3. **Arrow operations** - Columnar data manipulation
4. **Hash computation** - XxHash (default) or SHA256 hashing for value change detection

## Troubleshooting

### Flamegraphs Not Generated

1. Check pprof dependencies in `Cargo.toml`:
   ```toml
   pprof = { version = "0.13", features = ["flamegraph", "criterion"] }
   ```

2. Ensure `--profile-time` flag is used:
   ```bash
   cargo bench --bench bitemporal_benchmarks medium_dataset -- --profile-time 5
   ```

3. Verify criterion group configuration includes profiler

### GitHub Pages Not Updating

1. Check workflow permissions in repository settings
2. Verify GitHub Actions has completed successfully
3. Confirm Pages source is set to "GitHub Actions"

### HTML Links Not Working

1. Run the HTML post-processing script:
   ```bash
   python3 scripts/add_flamegraphs_to_html.py
   ```

2. Check that flamegraph.svg files exist in profile directories

## Future Enhancements

Potential improvements:

1. **Interactive flamegraphs** with zoom/search capabilities
2. **Historical comparison** across commits  
3. **Performance regression detection** with alerts
4. **Custom styling** for better visual integration
5. **Multiple profiling tools** (perf, valgrind, etc.)

This automated benchmark publishing system provides comprehensive performance visibility for the bitemporal timeseries algorithm with minimal maintenance overhead.