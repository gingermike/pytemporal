# Benchmark Publishing to GitHub Pages

This document explains how the project automatically generates and publishes performance benchmarks to GitHub Pages.

## Live Benchmark URLs

- **üìà Dashboard**: https://gingermike.github.io/pytemporal/
- **üìä Criterion Reports**: https://gingermike.github.io/pytemporal/criterion/report/

## Overview

The benchmark publishing system consists of:

1. **Rust Benchmarks** - Criterion.rs with pprof flamegraph integration
2. **Python Benchmarks** - pytest-benchmark mirroring Rust test scenarios
3. **GitHub Actions Workflow** - Automated benchmark execution and publishing
4. **github-action-benchmark** - Historical trend tracking with regression alerts
5. **GitHub Pages** - Hosting for reports and dashboards

## Components

### 1. Benchmark Configuration

The benchmarks are configured in `benches/bitemporal_benchmarks.rs` with pprof integration:

```rust
use pprof::criterion::{Output, PProfProfiler};

fn profiled() -> Criterion {
    Criterion::default().with_profiler(PProfProfiler::new(100, Output::Flamegraph(None)))
}

criterion_group! {
    name = benches;
    config = profiled();
    targets = bench_small_dataset, bench_medium_dataset, // ...
}
```

### 2. Flamegraph Generation

Flamegraphs are generated by running benchmarks with the `--profile-time` flag:

```bash
# Generate flamegraph for medium dataset (5 seconds of profiling)
cargo bench --bench bitemporal_benchmarks medium_dataset -- --profile-time 5
```

Output location: `target/criterion/<benchmark_name>/profile/flamegraph.svg`

### 3. HTML Integration Script

The `scripts/add_flamegraphs_to_html.py` script post-processes Criterion's HTML reports to add flamegraph links:

```python
# Adds flamegraph links to the "Additional Plots" section
python3 scripts/add_flamegraphs_to_html.py target/criterion
```

This modifies the HTML reports to include:
```html
<li>
    <a href="../profile/flamegraph.svg">üî• Flamegraph</a>
</li>
```

### 4. GitHub Actions Workflow

The benchmarks are part of `.github/workflows/build-wheels.yml` and run on version tags:

1. **Runs Rust benchmarks** with flamegraph generation for key tests
2. **Runs Python benchmarks** via pytest-benchmark
3. **Post-processes HTML** to add flamegraph links and PyTemporal branding
4. **Tracks historical trends** via github-action-benchmark
5. **Publishes to GitHub Pages** with criterion reports

### 5. Python Benchmarks (pytest-benchmark)

Python benchmarks mirror the Rust test scenarios in `benches/test_python_benchmarks.py`:

```bash
# Run Python benchmarks locally
uv run python -m pytest benches/test_python_benchmarks.py --benchmark-only -v

# Save JSON output for CI
uv run python -m pytest benches/test_python_benchmarks.py --benchmark-only --benchmark-json=results.json
```

Python benchmark classes:
- `TestSmallDataset` - Small dataset (5 records)
- `TestMediumDataset` - Medium dataset (100 records)
- `TestScalingBySize` - Scaling tests (10 ‚Üí 500k records)
- `TestParallelEffectiveness` - ID distribution scenarios
- `TestWideDatasets` - Many columns (production-like)
- `TestUpdateModes` - Delta vs full_state comparison
- `TestConflationEffectiveness` - Adjacent segment merging

### 6. Benchmark Categories

The system generates reports for:

- **Core Benchmarks**:
  - `small_dataset` - Basic functionality (5 records)
  - `medium_dataset` - Medium scale (100 records) + flamegraph
  - `conflation_effectiveness` - Adjacent segment merging + flamegraph

- **Scaling Analysis**:
  - Dataset size scaling (10 ‚Üí 500k records)
  - Large dataset flamegraph (500k records)
  - Parallel effectiveness analysis

## Local Testing

### Generate Benchmarks Locally

```bash
# Generate flamegraphs for key benchmarks
cargo bench --bench bitemporal_benchmarks medium_dataset -- --profile-time 5
cargo bench --bench bitemporal_benchmarks conflation_effectiveness -- --profile-time 5
cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/500000" -- --profile-time 5

# Add flamegraph links to HTML reports
python3 scripts/add_flamegraphs_to_html.py

# Serve locally for testing
python3 -m http.server 8000 --directory target/criterion
```

Then visit: http://localhost:8000/report/

### Test GitHub Pages Setup

```bash
# Prepare pages directory (mimics GitHub Actions)
mkdir -p pages
cp -r target/criterion/* pages/

# Serve GitHub Pages content locally
python3 -m http.server 8000 --directory pages
```

## GitHub Pages Setup

### Prerequisites

1. **Enable GitHub Pages** in repository settings:
   - Go to Settings ‚Üí Pages
   - Source: "GitHub Actions"

2. **Required permissions** in workflow:
   ```yaml
   permissions:
     contents: read
     pages: write
     id-token: write
   ```

### Workflow Triggers

The benchmark workflow runs on:
- **Version tags** (`v*`) - Full benchmarks with historical tracking
- Manual workflow dispatch

Benchmarks only run on tag releases to maintain clean historical trend data.

### Expected Output

After successful deployment, GitHub Pages will host:

- **Landing page** at `https://<username>.github.io/<repo>/`
- **Historical trends** tracked in `data.js`
- **Criterion reports** at `criterion/` with integrated flamegraph links
- **Flamegraph SVG files** directly accessible

## Performance Expectations

Based on current benchmarking:

| Benchmark | Expected Time | Flamegraph |
|-----------|---------------|------------|
| Small Dataset (5 records) | ~30-35 ¬µs | ‚ùå |
| Medium Dataset (100 records) | ~165-170 ¬µs | ‚úÖ |
| Large Dataset (500k records) | ~900-950 ms | ‚úÖ |
| Conflation Effectiveness | ~28 ¬µs | ‚úÖ |

## Flamegraph Analysis

Flamegraphs help identify:

- **Hot functions**: Wide bars show time-consuming operations
- **Call patterns**: Stack depth shows function call chains  
- **Parallel effectiveness**: Compare serial vs parallel sections
- **Memory allocations**: Identify allocation hotspots

### Key Areas to Monitor

1. **`process_id_timeline`** - Core algorithm logic
2. **Rayon parallel processing** - Parallelization overhead
3. **Arrow operations** - Columnar data manipulation
4. **Hash computation** - XxHash (default) or SHA256 hashing for value change detection

## Troubleshooting

### Flamegraphs Not Generated

1. Check pprof dependencies in `Cargo.toml`:
   ```toml
   pprof = { version = "0.13", features = ["flamegraph", "criterion"] }
   ```

2. Ensure `--profile-time` flag is used:
   ```bash
   cargo bench --bench bitemporal_benchmarks medium_dataset -- --profile-time 5
   ```

3. Verify criterion group configuration includes profiler

### GitHub Pages Not Updating

1. Check workflow permissions in repository settings
2. Verify GitHub Actions has completed successfully
3. Confirm Pages source is set to "GitHub Actions"

### HTML Links Not Working

1. Run the HTML post-processing script:
   ```bash
   python3 scripts/add_flamegraphs_to_html.py
   ```

2. Check that flamegraph.svg files exist in profile directories

## Future Enhancements

Potential improvements:

1. **Interactive flamegraphs** with zoom/search capabilities
2. ~~**Historical comparison** across commits~~ ‚úÖ Implemented via github-action-benchmark
3. ~~**Performance regression detection** with alerts~~ ‚úÖ Implemented (150% threshold)
4. ~~**Custom styling** for better visual integration~~ ‚úÖ Implemented (PyTemporal branding)
5. **Multiple profiling tools** (perf, valgrind, etc.)

This automated benchmark publishing system provides comprehensive performance visibility for the bitemporal timeseries algorithm with minimal maintenance overhead.