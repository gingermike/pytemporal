name: Build and Publish Linux Wheels

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
    - name: Install uv
      run: curl -LsSf https://astral.sh/uv/install.sh | sh
    - name: Create virtual environment
      run: uv venv
    - name: Install maturin
      run: uv pip install maturin[patchelf]
    - name: Run Rust tests
      run: cargo test --verbose
    - name: Install Python dependencies and run Python tests
      run: |
        uv pip install pytest pandas pyarrow
        uv run maturin develop
        uv run python -m pytest tests/test_bitemporal.py -v
    - name: Build release binary (smoke test)
      run: |
        # Build release binary to ensure it compiles correctly
        cargo build --release

  build-linux-wheels:
    needs: test
    runs-on: ubuntu-latest
    strategy:
      matrix:
        target: [x86_64]
        python-version: ['3.9', '3.10', '3.11', '3.12']
    steps:
    - uses: actions/checkout@v4
    
    - name: Extract version from tag
      id: version
      run: |
        if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
          VERSION=${GITHUB_REF#refs/tags/v}
          echo "VERSION=$VERSION" >> $GITHUB_OUTPUT
          echo "Building version: $VERSION"
        else
          echo "VERSION=0.1.0-dev" >> $GITHUB_OUTPUT
          echo "Building development version"
        fi
        
    - name: Update version in files
      if: startsWith(github.ref, 'refs/tags/v')
      run: |
        VERSION=${{ steps.version.outputs.VERSION }}
        echo "Updating version to: $VERSION"
        
        # Update Cargo.toml
        sed -i "s/^version = \".*\"/version = \"$VERSION\"/" Cargo.toml
        
        # Update pyproject.toml
        sed -i "s/^version = \".*\"/version = \"$VERSION\"/" pyproject.toml
        
        # Verify changes
        echo "Updated Cargo.toml:"
        grep "^version" Cargo.toml
        echo "Updated pyproject.toml:"
        grep "^version" pyproject.toml
        
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        targets: ${{ matrix.target }}-unknown-linux-gnu
    - name: Install uv
      run: curl -LsSf https://astral.sh/uv/install.sh | sh
    - name: Create virtual environment
      run: uv venv
    - name: Install maturin
      run: uv pip install maturin[patchelf]
    - name: Build wheel
      run: |
        # Use the specific Python version set up by actions/setup-python
        PYTHON_PATH=$(which python3)
        echo "Using Python interpreter: $PYTHON_PATH (${{ matrix.python-version }})"
        python3 --version
        
        # Build wheel for this specific Python version
        uv run maturin build --release --out dist --interpreter $PYTHON_PATH
    - name: List built wheels
      run: ls -la dist/
    - name: Upload wheel as artifact
      uses: actions/upload-artifact@v4
      with:
        name: wheels-linux-${{ matrix.target }}-py${{ matrix.python-version }}
        path: dist/*.whl

  publish-release:
    needs: [build-linux-wheels, benchmarks]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write
    steps:
    - uses: actions/checkout@v4
    
    - name: Extract version from tag
      id: version
      run: |
        VERSION=${GITHUB_REF#refs/tags/v}
        echo "VERSION=$VERSION" >> $GITHUB_OUTPUT
        echo "FULL_TAG=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT
    
    - name: Download all wheel artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: wheels-linux-*
        path: dist-all/
        
    - name: Download benchmark artifacts
      uses: actions/download-artifact@v4
      with:
        name: benchmarks-${{ steps.version.outputs.FULL_TAG }}
        path: benchmark-artifacts/
        
    - name: Flatten wheel directory
      run: |
        mkdir -p dist/
        find dist-all/ -name "*.whl" -exec cp {} dist/ \;
        ls -la dist/
        
    - name: Prepare release assets
      run: |
        mkdir -p release-assets/
        # Copy wheels
        cp dist/*.whl release-assets/
        # Copy benchmark ZIP
        find benchmark-artifacts/ -name "*.zip" -exec cp {} release-assets/ \;
        ls -la release-assets/
        
    - name: Upload GitHub Release
      uses: softprops/action-gh-release@v1
      with:
        tag_name: ${{ github.ref_name }}
        name: Release ${{ steps.version.outputs.VERSION }}
        files: release-assets/*
        generate_release_notes: true
        body: |
          ## Installation
          ```bash
          pip install --no-index --find-links="https://github.com/${{ github.repository }}/releases/download/${{ github.ref_name }}" pytemporal==${{ steps.version.outputs.VERSION }}
          ```

          ## Performance Documentation
          📊 **[View Interactive Benchmark Reports](https://gingermike.github.io/pytemporal/)**
          
          📦 **Download Complete Benchmark Data**: `benchmarks-${{ steps.version.outputs.FULL_TAG }}.zip` (includes flamegraphs)

          ## Linux Wheels Built
          - x86_64 (Intel/AMD 64-bit) for Python 3.9, 3.10, 3.11, 3.12

          ## Features
          - High-performance bitemporal timeseries processing  
          - Microsecond precision timestamps for audit trails
          - Conflation optimization for reduced storage
          - Adaptive parallelisation for large datasets
          
          ## Benchmark Results (v${{ steps.version.outputs.VERSION }})
          - Small Dataset (5 records): ~30-35 µs
          - Medium Dataset (100 records): ~165-170 µs  
          - Large Dataset (500k records): ~900-950 ms
          - Conflation Effectiveness: ~28 µs
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  benchmarks:
    needs: test
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    steps:
    - uses: actions/checkout@v4
    
    - name: Extract version from tag
      id: version
      run: |
        VERSION=${GITHUB_REF#refs/tags/v}
        echo "VERSION=$VERSION" >> $GITHUB_OUTPUT
        echo "FULL_TAG=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT
        echo "Building benchmarks for version: $VERSION"
        
    - name: Update version in files
      run: |
        VERSION=${{ steps.version.outputs.VERSION }}
        echo "Updating version to: $VERSION"
        
        # Update Cargo.toml
        sed -i "s/^version = \".*\"/version = \"$VERSION\"/" Cargo.toml
        
        # Update pyproject.toml
        sed -i "s/^version = \".*\"/version = \"$VERSION\"/" pyproject.toml
        
        # Verify changes
        echo "Updated Cargo.toml:"
        grep "^version" Cargo.toml
        echo "Updated pyproject.toml:"
        grep "^version" pyproject.toml
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        toolchain: stable
        
    - name: Install uv
      run: curl -LsSf https://astral.sh/uv/install.sh | sh
      
    - name: Install Python dependencies
      run: |
        python3 -m pip install --upgrade pip
        
    - name: Build project
      run: cargo build --release
      
    - name: Run benchmarks and generate flamegraphs
      run: |
        echo "🔥 Running core benchmarks with flamegraph generation..."
        
        # Generate flamegraphs for key performance analysis benchmarks
        cargo bench --bench bitemporal_benchmarks medium_dataset -- --profile-time 3
        cargo bench --bench bitemporal_benchmarks conflation_effectiveness -- --profile-time 3
        
        # Large dataset benchmark with shorter profiling to avoid timeout
        cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/500000" -- --profile-time 2
        
        
        # Run smaller benchmarks without flamegraphs for speed
        cargo bench --bench bitemporal_benchmarks small_dataset
        cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/10"
        cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/50"
        cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/100"
        cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/500"
        
        # Run all parallel effectiveness scenarios
        cargo bench --bench bitemporal_benchmarks "parallel_effectiveness/scenario/few_ids_many_records"
        cargo bench --bench bitemporal_benchmarks "parallel_effectiveness/scenario/many_ids_few_records"
        cargo bench --bench bitemporal_benchmarks "parallel_effectiveness/scenario/balanced_workload"
        
        echo "✅ All benchmarks completed"
        
    - name: Add flamegraph links to HTML reports
      run: |
        echo "🔗 Adding flamegraph links to HTML reports..."
        python3 scripts/add_flamegraphs_to_html.py
        echo "✅ HTML reports updated with flamegraph links"

    - name: Create benchmark ZIP artifact
      run: |
        echo "📦 Creating benchmark ZIP archive..."
        mkdir -p benchmark-artifacts
        cd target/criterion
        zip -r ../../benchmark-artifacts/benchmarks-${{ steps.version.outputs.FULL_TAG }}.zip .
        cd ../..
        ls -la benchmark-artifacts/
        echo "✅ Benchmark ZIP created"

    - name: Upload benchmark ZIP artifact
      uses: actions/upload-artifact@v4
      with:
        name: benchmarks-${{ steps.version.outputs.FULL_TAG }}
        path: benchmark-artifacts/*.zip

    - name: Prepare GitHub Pages content
      run: |
        echo "📦 Preparing GitHub Pages content..."
        
        # Copy criterion reports to a pages directory
        mkdir -p site
        cp -r target/criterion/* site/
        
        # Create a landing page with version information
        cat > site/index.html << EOF
        <!DOCTYPE html>
        <html>
        <head>
            <title>Bitemporal Timeseries v${{ steps.version.outputs.VERSION }} - Performance Benchmarks</title>
            <meta charset="utf-8">
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
                .container { max-width: 1200px; margin: 0 auto; }
                h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }
                h2 { color: #34495e; margin-top: 30px; }
                .benchmark-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }
                .benchmark-card { border: 1px solid #ddd; border-radius: 8px; padding: 20px; background: #f9f9f9; }
                .benchmark-card h3 { margin-top: 0; color: #2980b9; }
                .benchmark-links { margin: 10px 0; }
                .benchmark-links a { display: inline-block; margin: 5px 10px 5px 0; padding: 8px 12px; background: #3498db; color: white; text-decoration: none; border-radius: 4px; font-size: 14px; }
                .benchmark-links a:hover { background: #2980b9; }
                .flamegraph-link { background: #e74c3c !important; }
                .flamegraph-link:hover { background: #c0392b !important; }
                .summary { background: #ecf0f1; padding: 20px; border-radius: 8px; margin: 20px 0; }
                .footer { margin-top: 40px; padding-top: 20px; border-top: 1px solid #bdc3c7; text-align: center; color: #7f8c8d; }
                .download-section { background: #e8f5e8; padding: 15px; border-radius: 8px; margin: 20px 0; }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>🚀 PyTemporal v${{ steps.version.outputs.VERSION }} Performance Benchmarks</h1>
                
                <div class="summary">
                    <h2>📊 Performance Summary</h2>
                    <p><strong>Release:</strong> ${{ steps.version.outputs.FULL_TAG }} | <strong>Generated:</strong> $(date -u '+%Y-%m-%d %H:%M UTC')</p>
                    <p>This page contains comprehensive performance benchmarks for the pytemporal bitemporal timeseries processing algorithm, including flamegraphs for detailed performance analysis.</p>
                    <ul>
                        <li><strong>Small Dataset (5 records)</strong>: ~30-35 µs</li>
                        <li><strong>Medium Dataset (100 records)</strong>: ~165-170 µs</li> 
                        <li><strong>Large Dataset (500k records)</strong>: ~900-950 ms</li>
                        <li><strong>Conflation Effectiveness</strong>: ~28 µs</li>
                    </ul>
                </div>

                <div class="download-section">
                    <h2>📥 Download Complete Benchmark Data</h2>
                    <p>Download the complete benchmark results including all raw data, HTML reports, and flamegraphs:</p>
                    <a href="https://github.com/${{ github.repository }}/releases/download/${{ steps.version.outputs.FULL_TAG }}/benchmarks-${{ steps.version.outputs.FULL_TAG }}.zip" style="display: inline-block; padding: 10px 20px; background: #27ae60; color: white; text-decoration: none; border-radius: 5px; font-weight: bold;">
                        📦 Download benchmarks-${{ steps.version.outputs.FULL_TAG }}.zip
                    </a>
                </div>

                <h2>🎯 Core Benchmarks</h2>
                <div class="benchmark-grid">
                    <div class="benchmark-card">
                        <h3>Small Dataset</h3>
                        <p>Basic functionality test with 5 records</p>
                        <div class="benchmark-links">
                            <a href="small_dataset/report/">📈 Report</a>
                        </div>
                    </div>
                    
                    <div class="benchmark-card">
                        <h3>Medium Dataset</h3>
                        <p>Medium-scale test with 100 current records and 20 updates</p>
                        <div class="benchmark-links">
                            <a href="medium_dataset/report/">📈 Report</a>
                            <a href="medium_dataset/profile/flamegraph.svg" class="flamegraph-link">🔥 Flamegraph</a>
                        </div>
                    </div>
                    
                    <div class="benchmark-card">
                        <h3>Conflation Effectiveness</h3>
                        <p>Tests the effectiveness of adjacent segment merging</p>
                        <div class="benchmark-links">
                            <a href="conflation_effectiveness/report/">📈 Report</a>
                            <a href="conflation_effectiveness/profile/flamegraph.svg" class="flamegraph-link">🔥 Flamegraph</a>
                        </div>
                    </div>
                </div>

                <h2>📏 Scaling Analysis</h2>
                <div class="benchmark-grid">
                    <div class="benchmark-card">
                        <h3>Dataset Size Scaling</h3>
                        <p>Performance analysis across different dataset sizes</p>
                        <div class="benchmark-links">
                            <a href="scaling_by_dataset_size/report/">📈 Overview</a>
                            <a href="scaling_by_dataset_size/records/10/report/">10 records</a>
                            <a href="scaling_by_dataset_size/records/50/report/">50 records</a>
                            <a href="scaling_by_dataset_size/records/100/report/">100 records</a>
                            <a href="scaling_by_dataset_size/records/500/report/">500 records</a>
                            <a href="scaling_by_dataset_size/records/500000/report/">500k records</a>
                        </div>
                        <div class="benchmark-links">
                            <a href="scaling_by_dataset_size/records/500000/profile/flamegraph.svg" class="flamegraph-link">🔥 Large Dataset Flamegraph</a>
                        </div>
                    </div>
                    
                    <div class="benchmark-card">
                        <h3>Parallel Effectiveness</h3>
                        <p>Analysis of parallelization effectiveness across different workload patterns</p>
                        <div class="benchmark-links">
                            <a href="parallel_effectiveness/report/">📈 Overview</a>
                            <a href="parallel_effectiveness/scenario/few_ids_many_records/report/">Few IDs, Many Records</a>
                            <a href="parallel_effectiveness/scenario/many_ids_few_records/report/">Many IDs, Few Records</a>
                            <a href="parallel_effectiveness/scenario/balanced_workload/report/">Balanced Workload</a>
                        </div>
                    </div>
                </div>

                <h2>🔥 Understanding Flamegraphs</h2>
                <div class="summary">
                    <p><strong>Flamegraphs</strong> show exactly where your code spends time:</p>
                    <ul>
                        <li><strong>Width</strong>: How much time is spent in each function</li>
                        <li><strong>Height</strong>: Call stack depth</li>
                        <li><strong>Colors</strong>: Different functions/modules</li>
                        <li><strong>Click</strong>: Zoom into specific functions</li>
                    </ul>
                    <p>Look for wide bars to identify performance hotspots in the bitemporal algorithm.</p>
                </div>

                <div class="footer">
                    <p>Generated by <a href="https://github.com/bheisler/criterion.rs">Criterion.rs</a> with flamegraph integration via <a href="https://github.com/tikv/pprof-rs">pprof-rs</a></p>
                    <p><strong>Version:</strong> ${{ steps.version.outputs.FULL_TAG }} | <strong>Generated:</strong> $(date -u '+%Y-%m-%d %H:%M UTC')</p>
                    <p>View other releases: <a href="https://github.com/${{ github.repository }}/releases">Release History</a></p>
                </div>
            </div>
        </body>
        </html>
        EOF
        
        echo "✅ GitHub Pages content prepared"
        
    - name: Upload pages artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: site

  deploy-benchmarks:
    needs: benchmarks
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
    - name: Deploy to GitHub Pages
      uses: actions/deploy-pages@v4   # upgraded to v4
      id: deployment

  publish-pypi:
    needs: build-linux-wheels  # or 'needs: publish-release' if you want PyPI only after GitHub release
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/pytemporal
    permissions:
      id-token: write
    if: startsWith(github.ref, 'refs/tags/v')  # only on version tags
    steps:
      - name: Check out repository (needed by pypa publish action)
        uses: actions/checkout@v4

      - name: Download all built wheel artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: wheels-linux-*
          path: dist-all/
          
      - name: Flatten wheel directory for PyPI
        run: |
          mkdir -p dist/
          find dist-all/ -name "*.whl" -exec cp {} dist/ \;
          ls -la dist/

      - name: Publish package to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          packages_dir: dist/
          skip_existing: true