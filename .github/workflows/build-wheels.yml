name: Build and Publish Linux Wheels

on:
  push:
    tags:
      - 'v*'
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
    - name: Install uv
      run: curl -LsSf https://astral.sh/uv/install.sh | sh
    - name: Create virtual environment
      run: uv venv
    - name: Install maturin
      run: uv pip install maturin[patchelf]
    - name: Run Rust tests
      run: cargo test --verbose
    - name: Install Python dependencies and run Python tests
      run: |
        uv pip install pytest pandas pyarrow
        uv run maturin develop
        uv run python -m pytest tests/test_bitemporal.py -v
    - name: Build release binary (smoke test)
      run: |
        # Build release binary to ensure it compiles correctly
        cargo build --release

  build-linux-wheels:
    needs: test
    runs-on: ubuntu-latest
    strategy:
      matrix:
        target: [x86_64]
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        targets: ${{ matrix.target }}-unknown-linux-gnu
    - name: Install uv
      run: curl -LsSf https://astral.sh/uv/install.sh | sh
    - name: Create virtual environment
      run: uv venv
    - name: Install maturin
      run: uv pip install maturin[patchelf]
    - name: Build wheel
      run: |
        PYTHON_PATH=$(which python3)
        echo "Using Python interpreter: $PYTHON_PATH"
        uv run maturin build --release --out dist --find-interpreter
    - name: Upload wheel as artifact
      uses: actions/upload-artifact@v4
      with:
        name: wheels-linux-${{ matrix.target }}
        path: dist/*.whl

  publish-release:
    needs: [build-linux-wheels, benchmarks]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      contents: write
    steps:
    - uses: actions/checkout@v4
    
    - name: Extract version from tag
      id: version
      run: |
        VERSION=${GITHUB_REF#refs/tags/v}
        echo "VERSION=$VERSION" >> $GITHUB_OUTPUT
        echo "FULL_TAG=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT
    
    - name: Download wheel artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: wheels-*
        path: dist-all/
        
    - name: Download benchmark artifacts
      uses: actions/download-artifact@v4
      with:
        name: benchmarks-${{ steps.version.outputs.FULL_TAG }}
        path: benchmark-artifacts/
        
    - name: Prepare release assets
      run: |
        mkdir -p release-assets/
        # Copy wheels
        find dist-all/ -name "*.whl" -exec cp {} release-assets/ \;
        # Copy benchmark ZIP
        find benchmark-artifacts/ -name "*.zip" -exec cp {} release-assets/ \;
        ls -la release-assets/
        
    - name: Upload GitHub Release
      uses: softprops/action-gh-release@v1
      with:
        tag_name: ${{ github.ref_name }}
        name: Release ${{ steps.version.outputs.VERSION }}
        files: release-assets/*
        generate_release_notes: true
        body: |
          ## Installation
          ```bash
          pip install --no-index --find-links="https://github.com/${{ github.repository }}/releases/download/${{ github.ref_name }}" bitemporal-timeseries==${{ steps.version.outputs.VERSION }}
          ```

          ## Performance Documentation
          üìä **[View Interactive Benchmark Reports](https://your-username.github.io/bitemporal-timeseries/)**
          
          üì¶ **Download Complete Benchmark Data**: `benchmarks-${{ steps.version.outputs.FULL_TAG }}.zip` (includes flamegraphs)

          ## Features
          - High-performance bitemporal timeseries processing  
          - Microsecond precision timestamps for audit trails
          - Conflation optimization for reduced storage
          - Adaptive parallelisation for large datasets
          
          ## Benchmark Results (v${{ steps.version.outputs.VERSION }})
          - Small Dataset (5 records): ~30-35 ¬µs
          - Medium Dataset (100 records): ~165-170 ¬µs  
          - Large Dataset (500k records): ~900-950 ms
          - Conflation Effectiveness: ~28 ¬µs
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  benchmarks:
    needs: test
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    steps:
    - uses: actions/checkout@v4
    
    - name: Extract version from tag
      id: version
      run: |
        VERSION=${GITHUB_REF#refs/tags/v}
        echo "VERSION=$VERSION" >> $GITHUB_OUTPUT
        echo "FULL_TAG=${GITHUB_REF#refs/tags/}" >> $GITHUB_OUTPUT
        echo "Building benchmarks for version: $VERSION"
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
      with:
        toolchain: stable
        
    - name: Install uv
      run: curl -LsSf https://astral.sh/uv/install.sh | sh
      
    - name: Install Python dependencies
      run: |
        python3 -m pip install --upgrade pip
        
    - name: Build project
      run: cargo build --release
      
    - name: Run benchmarks and generate flamegraphs
      run: |
        echo "üî• Running core benchmarks with flamegraph generation..."
        
        # Generate flamegraphs for key performance analysis benchmarks
        cargo bench --bench bitemporal_benchmarks medium_dataset -- --profile-time 3
        cargo bench --bench bitemporal_benchmarks conflation_effectiveness -- --profile-time 3
        
        # Large dataset benchmark with shorter profiling to avoid timeout
        timeout 300s cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/500000" -- --profile-time 2 || echo "‚ö†Ô∏è Large benchmark timed out - continuing"
        
        echo "üìä Running additional benchmarks for completeness..."
        
        # Run smaller benchmarks without flamegraphs for speed
        cargo bench --bench bitemporal_benchmarks small_dataset
        cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/100"
        cargo bench --bench bitemporal_benchmarks "scaling_by_dataset_size/records/500"
        
        # Run parallel effectiveness tests
        cargo bench --bench bitemporal_benchmarks "parallel_effectiveness/scenario/balanced_workload"
        
        echo "‚úÖ All benchmarks completed"
        
    - name: Add flamegraph links to HTML reports
      run: |
        echo "üîó Adding flamegraph links to HTML reports..."
        python3 scripts/add_flamegraphs_to_html.py
        echo "‚úÖ HTML reports updated with flamegraph links"

    - name: Create benchmark ZIP artifact
      run: |
        echo "üì¶ Creating benchmark ZIP archive..."
        mkdir -p benchmark-artifacts
        cd target/criterion
        zip -r ../../benchmark-artifacts/benchmarks-${{ steps.version.outputs.FULL_TAG }}.zip .
        cd ../..
        ls -la benchmark-artifacts/
        echo "‚úÖ Benchmark ZIP created"

    - name: Upload benchmark ZIP artifact
      uses: actions/upload-artifact@v4
      with:
        name: benchmarks-${{ steps.version.outputs.FULL_TAG }}
        path: benchmark-artifacts/*.zip

    - name: Prepare GitHub Pages content
      run: |
        echo "üì¶ Preparing GitHub Pages content..."
        
        # Copy criterion reports to a pages directory
        mkdir -p site
        cp -r target/criterion/* site/
        
        # Create a landing page with version information
        cat > site/index.html << EOF
        <!DOCTYPE html>
        <html>
        <head>
            <title>Bitemporal Timeseries v${{ steps.version.outputs.VERSION }} - Performance Benchmarks</title>
            <meta charset="utf-8">
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
                .container { max-width: 1200px; margin: 0 auto; }
                h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }
                h2 { color: #34495e; margin-top: 30px; }
                .benchmark-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }
                .benchmark-card { border: 1px solid #ddd; border-radius: 8px; padding: 20px; background: #f9f9f9; }
                .benchmark-card h3 { margin-top: 0; color: #2980b9; }
                .benchmark-links { margin: 10px 0; }
                .benchmark-links a { display: inline-block; margin: 5px 10px 5px 0; padding: 8px 12px; background: #3498db; color: white; text-decoration: none; border-radius: 4px; font-size: 14px; }
                .benchmark-links a:hover { background: #2980b9; }
                .flamegraph-link { background: #e74c3c !important; }
                .flamegraph-link:hover { background: #c0392b !important; }
                .summary { background: #ecf0f1; padding: 20px; border-radius: 8px; margin: 20px 0; }
                .footer { margin-top: 40px; padding-top: 20px; border-top: 1px solid #bdc3c7; text-align: center; color: #7f8c8d; }
                .download-section { background: #e8f5e8; padding: 15px; border-radius: 8px; margin: 20px 0; }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>üöÄ Bitemporal Timeseries v${{ steps.version.outputs.VERSION }} Performance Benchmarks</h1>
                
                <div class="summary">
                    <h2>üìä Performance Summary</h2>
                    <p><strong>Release:</strong> ${{ steps.version.outputs.FULL_TAG }} | <strong>Generated:</strong> $(date -u '+%Y-%m-%d %H:%M UTC')</p>
                    <p>This page contains comprehensive performance benchmarks for the bitemporal timeseries processing algorithm, including flamegraphs for detailed performance analysis.</p>
                    <ul>
                        <li><strong>Small Dataset (5 records)</strong>: ~30-35 ¬µs</li>
                        <li><strong>Medium Dataset (100 records)</strong>: ~165-170 ¬µs</li> 
                        <li><strong>Large Dataset (500k records)</strong>: ~900-950 ms</li>
                        <li><strong>Conflation Effectiveness</strong>: ~28 ¬µs</li>
                    </ul>
                </div>

                <div class="download-section">
                    <h2>üì• Download Complete Benchmark Data</h2>
                    <p>Download the complete benchmark results including all raw data, HTML reports, and flamegraphs:</p>
                    <a href="https://github.com/${{ github.repository }}/releases/download/${{ steps.version.outputs.FULL_TAG }}/benchmarks-${{ steps.version.outputs.FULL_TAG }}.zip" style="display: inline-block; padding: 10px 20px; background: #27ae60; color: white; text-decoration: none; border-radius: 5px; font-weight: bold;">
                        üì¶ Download benchmarks-${{ steps.version.outputs.FULL_TAG }}.zip
                    </a>
                </div>

                <h2>üéØ Core Benchmarks</h2>
                <div class="benchmark-grid">
                    <div class="benchmark-card">
                        <h3>Small Dataset</h3>
                        <p>Basic functionality test with 5 records</p>
                        <div class="benchmark-links">
                            <a href="small_dataset/report/">üìà Report</a>
                        </div>
                    </div>
                    
                    <div class="benchmark-card">
                        <h3>Medium Dataset</h3>
                        <p>Medium-scale test with 100 current records and 20 updates</p>
                        <div class="benchmark-links">
                            <a href="medium_dataset/report/">üìà Report</a>
                            <a href="medium_dataset/profile/flamegraph.svg" class="flamegraph-link">üî• Flamegraph</a>
                        </div>
                    </div>
                    
                    <div class="benchmark-card">
                        <h3>Conflation Effectiveness</h3>
                        <p>Tests the effectiveness of adjacent segment merging</p>
                        <div class="benchmark-links">
                            <a href="conflation_effectiveness/report/">üìà Report</a>
                            <a href="conflation_effectiveness/profile/flamegraph.svg" class="flamegraph-link">üî• Flamegraph</a>
                        </div>
                    </div>
                </div>

                <h2>üìè Scaling Analysis</h2>
                <div class="benchmark-grid">
                    <div class="benchmark-card">
                        <h3>Dataset Size Scaling</h3>
                        <p>Performance analysis across different dataset sizes</p>
                        <div class="benchmark-links">
                            <a href="scaling_by_dataset_size/report/">üìà Overview</a>
                            <a href="scaling_by_dataset_size/records/10/report/">10 records</a>
                            <a href="scaling_by_dataset_size/records/50/report/">50 records</a>
                            <a href="scaling_by_dataset_size/records/100/report/">100 records</a>
                            <a href="scaling_by_dataset_size/records/500/report/">500 records</a>
                            <a href="scaling_by_dataset_size/records/500000/report/">500k records</a>
                        </div>
                        <div class="benchmark-links">
                            <a href="scaling_by_dataset_size/records/500000/profile/flamegraph.svg" class="flamegraph-link">üî• Large Dataset Flamegraph</a>
                        </div>
                    </div>
                    
                    <div class="benchmark-card">
                        <h3>Parallel Effectiveness</h3>
                        <p>Analysis of parallelization effectiveness across different workload patterns</p>
                        <div class="benchmark-links">
                            <a href="parallel_effectiveness/report/">üìà Overview</a>
                            <a href="parallel_effectiveness/scenario/few_ids_many_records/report/">Few IDs, Many Records</a>
                            <a href="parallel_effectiveness/scenario/many_ids_few_records/report/">Many IDs, Few Records</a>
                            <a href="parallel_effectiveness/scenario/balanced_workload/report/">Balanced Workload</a>
                        </div>
                    </div>
                </div>

                <h2>üî• Understanding Flamegraphs</h2>
                <div class="summary">
                    <p><strong>Flamegraphs</strong> show exactly where your code spends time:</p>
                    <ul>
                        <li><strong>Width</strong>: How much time is spent in each function</li>
                        <li><strong>Height</strong>: Call stack depth</li>
                        <li><strong>Colors</strong>: Different functions/modules</li>
                        <li><strong>Click</strong>: Zoom into specific functions</li>
                    </ul>
                    <p>Look for wide bars to identify performance hotspots in the bitemporal algorithm.</p>
                </div>

                <div class="footer">
                    <p>Generated by <a href="https://github.com/bheisler/criterion.rs">Criterion.rs</a> with flamegraph integration via <a href="https://github.com/tikv/pprof-rs">pprof-rs</a></p>
                    <p><strong>Version:</strong> ${{ steps.version.outputs.FULL_TAG }} | <strong>Generated:</strong> $(date -u '+%Y-%m-%d %H:%M UTC')</p>
                    <p>View other releases: <a href="https://github.com/${{ github.repository }}/releases">Release History</a></p>
                </div>
            </div>
        </body>
        </html>
        EOF
        
        echo "‚úÖ GitHub Pages content prepared"
        
    - name: Upload pages artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: site

  deploy-benchmarks:
    needs: benchmarks
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
    - name: Deploy to GitHub Pages
      uses: actions/deploy-pages@v4   # upgraded to v4
      id: deployment

  publish-pypi:
    needs: build-linux-wheels  # or 'needs: publish-release' if you want PyPI only after GitHub release
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/bitemporal-timeseries
    permissions:
      id-token: write
    if: startsWith(github.ref, 'refs/tags/v')  # only on version tags
    steps:
      - name: Check out repository (needed by pypa publish action)
        uses: actions/checkout@v4

      - name: Download built wheel artifacts
        uses: actions/download-artifact@v4
        with:
          name: wheels-linux-x86_64   # match the artifact name from build-linux-wheels
          path: dist/

      - name: Publish package to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          packages_dir: dist/
          skip_existing: true